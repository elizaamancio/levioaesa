import numpy as np
import pylab as plt


goal = 10

# map cell to cell, add circular cell to goal point

points_list = [(1,0),
(3,0),
(6,0),
(0,1),
(2,1),
(3,1),
(4,1),
(6,1),
(0,2),
(1,2),
(6,2),
(0,3),
(1,3),
(4,3),
(6,3),
(3,4),
(6,4),
(8,5),
(10,5),
(0,6),
(1,6),
(2,6),
(3,6),
(4,6),
(0,7),
(5,7),
(6,7),
(10,8),
(11,9),
(12,9),
(13,9),
(5,10),
(8,10),
(9,10),
(11,10),
(12,10),
(13,10),
(12,11),
(13,11),
(11,12),
(13,12),
(9,13),
(11,13),
(8,14),
(9,14),
(10,14),
(11,14),
(12,14),
(13,14),
(7,15),
(9,15),
(10,15),
(11,15),
(12,15),
(13,15)]

#goals_list = [(7, 5),(1,11), (9,13), (8,10),(14,5)]

# how many points in graph? x points
MATRIX_SIZE = 16


# create matrix x*y
R = np.matrix(np.ones(shape=(MATRIX_SIZE, MATRIX_SIZE)))
R *= -1

# how many points in graph? x points
MATRIX_SIZE = 16


# create matrix x*y
R = np.matrix(np.ones(shape=(MATRIX_SIZE, MATRIX_SIZE)))
R *= -1

R[(1,0)] = 1
R[(3,0)] = 1
R[(6,0)] = 1
R[(0,1)] = 1
R[(2,1)] = 1
R[(3,1)] = 1
R[(4,1)] = 1
R[(6,1)] = 1
R[(0,2)] = 1
R[(1,2)] = 1
R[(6,2)] = 1
R[(0,3)] = 1
R[(1,3)] = 1
R[(4,3)] = 1
R[(6,3)] = 1
R[(3,4)] = 1
R[(6,4)] = 1
R[(8,5)] = 1
R[(10,5)] = 1
R[(0,6)] = 1
R[(1,6)] = 1
R[(2,6)] = 1
R[(3,6)] = 1
R[(4,6)] = 1
R[(0,7)] = 1
R[(5,7)] = 1
R[(6,7)] = 1
R[(10,8)] = 1
R[(11,9)] = 1
R[(12,9)] = 1
R[(13,9)] = 1
R[(5,10)] = 1
R[(8,10)] = 1
R[(9,10)] = 1
R[(11,10)] = 1
R[(12,10)] = 1
R[(13,10)] = 1
R[(12,11)] = 1
R[(13,11)] = 1
R[(11,12)] = 1
R[(13,12)] = 1
R[(9,13)] = 1
R[(11,13)] = 1
R[(8,14)] = 1
R[(9,14)] = 1
R[(10,14)] = 1
R[(11,14)] = 1
R[(12,14)] = 1
R[(13,14)] = 1
R[(7,15)] = 1
R[(9,15)] = 1
R[(10,15)] = 1
R[(11,15)] = 1
R[(12,15)] = 1
R[(13,15)] = 1
R[(14,15)] = 1
R[(7,15)] = 1
R[(15,14)] = 1


# assign zeros to paths and 200 to goal-reaching point


for point in points_list:
    print(point)
    if point[1] == goal:
        R[point] = 200

    if point[0] == goal:
        R[point[::-1]] = 200


print (R)


Q = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))

# learning parameter
gamma = 0.8

initial_state = 0

def available_actions(state):
    current_state_row = R[state,]
    av_act = np.where(current_state_row > 0)[1]
    return av_act

available_act = available_actions(initial_state)

def sample_next_action(available_actions_range):
    next_action = int(np.random.choice(available_act,1))
    return next_action

action = sample_next_action(available_act)

def update(current_state, action, gamma):

  max_index = np.where(Q[action,] == np.max(Q[action,]))[1]

  if max_index.shape[0] > 1:
      max_index = int(np.random.choice(max_index, size = 1))
  else:
      max_index = int(max_index)
  max_value = Q[action, max_index]

  Q[current_state, action] = R[current_state, action] + gamma * max_value
  
  if (np.max(Q) > 0):
    return(np.sum(Q/np.max(Q)*100))
  else:
    return (0)

update(initial_state, action, gamma)


# Training
scores = []
for i in range(2000):
    current_state = np.random.randint(0, int(Q.shape[0]))
    available_act = available_actions(current_state)
    action = sample_next_action(available_act)
    score = update(current_state,action,gamma)
    scores.append(score)
    

print("Trained Q matrix:")
print(Q/np.max(Q)*100)


# Testing

current_state = 15
print(current_state)
steps = [current_state]
i= 0

while i<9:
    i = i+ 1
    next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[1]

    for j in range(16):
      Q[j, next_step_index] = 0

    if next_step_index.shape[0] > 1:
        next_step_index = int(np.random.choice(next_step_index, size = 1))
    else:
        next_step_index = int(next_step_index)

    steps.append(next_step_index)
    current_state = next_step_index

print("Most efficient path from ")

print(goal)
print(steps)

plt.plot(scores)
plt.show()
