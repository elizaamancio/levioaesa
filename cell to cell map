import numpy as np
import pylab as plt
from numpy import random


# map cell to cell, add circular cell to goal point

points_list = [(0,1),
(0,3),
(0,6),
(0,15),
(1,0),
(1,2),
(1,3),
(1,4),
(1,6),
(1,11),
(2,0),
(2,1),
(2,6),
(3,0),
(3,1),
(3,4),
(3,6),
(4,3),
(4,6),
(5,8),
(5,10),
(6,0),
(6,1),
(6,2),
(6,3),
(6,4),
(7,0),
(7,5),
(7,6),
(8,10),
(9,11),
(9,12),
(9,13),
(10,5),
(10,8),
(10,9),
(10,11),
(10,12),
(10,13),
(11,12),
(11,13),
(12,11),
(12,13),
#(13,9),
(13,11),
(14,8),
(14,9),
(14,10),
(14,11),
(14,12),
(14,13),
(15,7),
(15,9),
(15,11),
#(15,12),
(15,13)
]

# how many points in graph? x points
MATRIX_SIZE = 16


# create matrix x*y
R = np.matrix(np.ones(shape=(MATRIX_SIZE, MATRIX_SIZE)))
R *= -1

# how many points in graph? x points
MATRIX_SIZE = 16


# create matrix x*y
R = np.matrix(np.ones(shape=(MATRIX_SIZE, MATRIX_SIZE)))
R *= -1

R[(0,1)] = 1
R[(0,3)] = 1
R[(0,6)] = 1
R[(0,15)] = 1
R[(1,0)] = 1
R[(1,2)] = 1
R[(1,3)] = 1
R[(1,4)] = 1
R[(1,6)] = 1
R[(1,11)] = 1
R[(1,11)] = 1
R[(2,0)] = 1
R[(2,1)] = 1
R[(2,6)] = 1
R[(3,0)] = 1
R[(3,1)] = 1
R[(3,4)] = 1
R[(3,6)] = 1
R[(4,3)] = 1
R[(4,6)] = 1
R[(5,8)] = 1
R[(5,10)] = 1
R[(6,0)] = 1
R[(6,1)] = 1
R[(6,2)] = 1
R[(6,3)] = 1
R[(6,4)] = 1
R[(7,0)] = 1
R[(7,5)] = 1
R[(7,6)] = 1
R[(8,10)] = 1
R[(8,15)] = 1
R[(9,11)] = 1
R[(9,12)] = 1
R[(9,13)] = 1
R[(10,5)] = 1
R[(10,8)] = 1
R[(10,9)] = 1
R[(10,11)] = 1
R[(10,12)] = 1
R[(10,13)] = 1
R[(11,12)] = 1
R[(11,13)] = 1
R[(12,11)] = 1
R[(12,13)] = 1
R[(13,9)] = 1
R[(13,11)] = 1
R[(14,8)] = 1
R[(14,9)] = 1
R[(14,10)] = 1
R[(14,11)] = 1
R[(14,12)] = 1
R[(14,13)] = 1
R[(15,7)] = 1
R[(15,9)] = 1
R[(15,11)] = 1
R[(15,12)] = 1
R[(15,13)] = 1


R[(15,14)] = 1
R[(14,15)] = 1


# assign zeros to paths and 200 to goal-reaching point


goals_list = [
(7, 5),
(1,11), 
(9,13), 
(8,10),
(14,5),
]

for point in goals_list:
    print(point)
    R[point] = 200
    

print (R)


Q = np.matrix(np.zeros([MATRIX_SIZE,MATRIX_SIZE]))

# learning parameter
gamma = 0.8
alpha = 0.5


initial_state = 0

def available_actions(state):
    current_state_row = R[state,]
    av_act = np.where(current_state_row > 0)[1]
    return av_act

available_act = available_actions(initial_state)

def sample_next_action(available_actions_range, state):
    next_action = int(np.random.choice(available_act,1))
    
    best_action = np.where(Q[state,] == np.max(Q[state,]))[1]
    alpha = random.randint(100)
    
    if best_action.shape[0] == 1 and alpha < 80:
        best_action = int(best_action)
        next_action = best_action

    return next_action

action = sample_next_action(available_act, initial_state)

def update(current_state, action, gamma):
  max_index = np.where(Q[action,] == np.max(Q[action,]))[1]

  if max_index.shape[0] > 1:
      max_index = int(np.random.choice(max_index, size = 1))
  else:
      max_index = int(max_index)

  QSA = Q[current_state, action]
  RSA = R[current_state, action]
  max_value = Q[action, max_index]
  
  lambdaa = 0.8
  Q[current_state, action] =  QSA + lambdaa *(RSA + gamma * max_value - QSA)
  
  
  if (np.max(Q) > 0):
    return(np.sum(Q/np.max(Q)*100))
    #return (int(Q[current_state, action]))
  else:
    return (0)

update(initial_state, action, gamma)


# Training
scores = []
for i in range(2000):
    current_state = np.random.randint(0, int(Q.shape[0]))
    available_act = available_actions(current_state)
    action = sample_next_action(available_act, current_state)
    score = update(current_state,action,gamma)
    scores.append(score)

print("Trained Q matrix:")
print(Q/np.max(Q)*100)

for point in points_list:
    print(point)
    print(Q[point])

# Testing

current_state = 0
print(current_state)
steps = [current_state]
i= 0



while i<20:
    i = i+ 1
    
    next_step_index = np.where(Q[current_state,] == np.max(Q[current_state,]))[1]
    
    
    if next_step_index.shape[0] > 1:
        next_step_index = int(np.random.choice(next_step_index, size = 1))
        print(next_step_index)
    else:
        next_step_index = int(next_step_index)
        print("max", Q[current_state, next_step_index])
        print(next_step_index)
        
    
    Q[current_state, next_step_index] = -1
    current_state = next_step_index
    steps.append(next_step_index)
    

print("Most efficient path from ")

print(steps)
